# Results

```{r}
#load the three datasets and tidyverse package
library(tidyverse)

### --- Load & clean Media data ---
media <- read_csv("data/movie_media_clean.csv", show_col_types = FALSE) |>
  mutate(Date = as.Date(Date))

# --- Load & clean Sector ETFs ---
sectors <- read_csv("data/sector_etfs_clean.csv", show_col_types = FALSE) |>
  mutate(Date = as.Date(Date))

# --- Load Box Office Data ---
box_office <- read_csv("data/box_office.csv", show_col_types = FALSE)
# use mutate(Date = ...) here only if box_office contains a date column

```

### Missing Value Analysis
```{r fig.width = 9, fig.height = 4}
library(redav)

p_media   <- plot_missing(media,   num_char = 4)
p_sectors <- plot_missing(sectors, num_char = 4)
p_box     <- plot_missing(box_office, num_char = 4)

p_media
p_sectors
p_box
```


```{r}
library(tidyverse)
library(scales)

# sectors
missing_summary_sectors <- sectors |>
  summarise(across(everything(), ~ mean(is.na(.)))) |>
  pivot_longer(everything(),
               names_to = "variable",
               values_to = "prop_missing")

# media
missing_summary_media <- media |>
  summarise(across(everything(), ~ mean(is.na(.)))) |>
  pivot_longer(everything(),
               names_to = "variable",
               values_to = "prop_missing")

# box office
missing_summary_box <- box_office |>
  summarise(across(everything(), ~ mean(is.na(.)))) |>
  pivot_longer(everything(),
               names_to = "variable",
               values_to = "prop_missing")
```


```{r}
library(tidyverse)
library(scales)

# example: sectors data (do the same for media, box_office)
missing_summary_sectors <- sectors |>
  summarise(across(everything(), ~ mean(is.na(.)))) |>
  pivot_longer(everything(),
               names_to = "variable",
               values_to = "prop_missing")

ggplot(missing_summary_sectors,
       aes(x = fct_reorder(variable, prop_missing),
           y = prop_missing)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  scale_y_continuous(
  labels = percent_format(accuracy = 0.1),
  breaks = c(0, 0.0025, 0.005, 0.0075, 0.01)  # adjust as you like
) +
  labs(
    title = "Percent of Missing Values by Variable (Sector ETFs)",
    x = "",
    y = "% of rows missing"
  ) +
  theme_minimal(base_size = 12)
```


```{r}
library(ggplot2)
library(forcats)
library(scales)

## Movie/media missingness
ggplot(missing_summary_media,
       aes(x = fct_reorder(variable, prop_missing),
           y = prop_missing)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  scale_y_continuous(
  labels = percent_format(accuracy = 0.1),
  breaks = c(0, 0.0025, 0.005, 0.0075, 0.01)  # adjust as you like
) +
  labs(
    title = "Percent of Missing Values by Variable (Movie/Media)",
    x = "",
    y = "% of rows missing"
  ) +
  theme_minimal(base_size = 12)
```

```{r}
## Box office missingness
ggplot(missing_summary_box,
       aes(x = fct_reorder(variable, prop_missing),
           y = prop_missing)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Percent of Missing Values by Variable (Box Office)",
    x = "",
    y = "% of rows missing"
  ) +
  theme_minimal(base_size = 12)
```


### 1. The number of theatre releases and box office revenue still fall flat compared to pre-COVID levels 
```{r}
p_box1 <- ggplot(box_office, aes(Year, Domestic_BoxOffice)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = scales::dollar) + 
  labs(
    title = "U.S. Theatrical Box Office Revenue by Year",
    subtitle = "Pre-COVID peak, COVID collapse, and incomplete recovery",
    x = "Year",
    y = "Total Domestic Box Office"
  ) +
  theme_minimal()

p_box1
```


```{r}
p_box2 <- ggplot(box_office, aes(Year, Releases)) +
  geom_col(fill = "#4C72B0") +
  labs(
    title = "Number of Theatrical Releases per Year",
    subtitle = "Industry output collapsed during COVID and remains structurally lower",
    x = "Year",
    y = "Number of releases"
  ) +
  theme_minimal()

p_box2
```


locations of movie theatres by state? 2019 and then 2024? can count numbers and use map for % of decrease?  https://investor.amctheatres.com/sec-filings/all-sec-filings/content/0001411579-25-000042/amc-20241231x10k.htm
https://investor.amctheatres.com/sec-filings/all-sec-filings/content/0001411579-20-000027/amc-20191231x10k.htm
```{r}
library(readxl)
library(dplyr)
library(stringr)
library(sf)
library(ggplot2)
library(scales)

## 1. Read AMC tables -------------------------------------------------------

amc_2019 <- read_excel(
  "data/AMC_theatres_screens_by_market_2019.xlsx",
  sheet = 1
) |>
  rename(
    state    = `U.S. Markets`,
    theatres = Theatres,
    screens  = Screens
  ) |>
  mutate(state = str_trim(state))

amc_2024 <- read_excel(
  "data/AMC_theatres_screens_by_market_2024.xlsx",
  sheet = 1
) |>
  rename(
    state    = `U.S. Markets`,
    theatres = Theatres,
    screens  = Screens
  ) |>
  mutate(state = str_trim(state))

## 2. Read shapefile + keep only contiguous U.S. ---------------------------

states_sf <- st_read("data/cb_2018_us_state_20m/cb_2018_us_state_20m.shp")

states_sf_conus <- states_sf |>
  filter(!STUSPS %in% c("AK", "HI", "PR", "GU", "VI", "AS", "MP")) |>
  mutate(state = NAME)

# vector of valid state names (lower 48 + DC)
valid_states <- states_sf_conus$state

## 3. Filter AMC tables to states in shapefile -----------------------------

amc_2019_clean <- amc_2019 |>
  filter(state %in% valid_states)

amc_2024_clean <- amc_2024 |>
  filter(state %in% valid_states)

## 4. Compute changes ------------------------------------------------------

state_changes <- amc_2019_clean |>
  select(
    state,
    theatres_2019 = theatres,
    screens_2019  = screens
  ) |>
  inner_join(
    amc_2024_clean |>
      select(
        state,
        theatres_2024 = theatres,
        screens_2024  = screens
      ),
    by = "state"
  ) |>
  mutate(
    theatres_change     = theatres_2024 - theatres_2019,
    theatres_pct_change = (theatres_2024 - theatres_2019) / theatres_2019,
    screens_change      = screens_2024 - screens_2019,
    screens_pct_change  = (screens_2024 - screens_2019) / screens_2019
  )

## 5. Join back to shapefile for mapping -----------------------------------

map_data <- states_sf_conus |>
  left_join(state_changes, by = "state")
```

```{r}
library(ggplot2)
library(scales)

ggplot(map_data) +
  geom_sf(aes(fill = theatres_pct_change),
          color = "white", size = 0.2) +
  # zoom to lower 48 + DC
  coord_sf(xlim = c(-125, -66), ylim = c(24, 50), expand = FALSE) +
  scale_fill_gradient2(
    name   = "Percent change\nin theatres",
    labels = percent_format(accuracy = 1),
    low    = "#b2182b",   # more negative (closures)
    mid    = "white",
    high   = "#2166ac",   # more positive (growth)
    midpoint = 0
  ) +
  labs(
    title    = "Change in AMC Theatres by State (2019–2024)",
    subtitle = "Percent change in number of theatres",
    caption  = "Source: AMC U.S. Markets Theatres & Screens tables, 2019 vs 2024"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title = element_blank()
  )
```

```{r}
ggplot(map_data) +
  geom_sf(aes(fill = screens_pct_change),
          color = "white", size = 0.2) +
  coord_sf(xlim = c(-125, -66), ylim = c(24, 50), expand = FALSE) +
  scale_fill_gradient2(
    name   = "Percent change\nin screens",
    labels = percent_format(accuracy = 1),
    low    = "#b2182b",   # big decreases
    mid    = "white",
    high   = "#2166ac",   # increases
    midpoint = 0
  ) +
  labs(
    title    = "Change in AMC Screens by State (2019–2024)",
    subtitle = "Percent change in number of screens",
    caption  = "Source: AMC U.S. Markets Theatres & Screens tables, 2019 vs 2024"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title = element_blank()
  )
```













### 2. While most U.S. sectors recovered strongly post-COVID, the movie industry lags all major sectors
```{r}
# Rebase each media company to 1
media_rebased <- media |>
  group_by(Ticker) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(base = first(Adj_Close),
         rebased = Adj_Close / base) |>
  ungroup()

# Compute equal-weight average index
movie_index <- media_rebased |>
  group_by(Date) |>
  summarize(
    Movie_Index = mean(rebased, na.rm = TRUE)
  )


sectors_rebased <- sectors |>
  group_by(Ticker, Sector) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    base = first(`Adj Close`),
    rebased = `Adj Close` / base
  ) |>
  ungroup()


# Sector ETF index
sector_index <- sectors_rebased |>
  select(Date, Sector, rebased)

# Movie Index needs a pseudo "Sector" label
movie_index2 <- movie_index |>
  mutate(Sector = "Movie Industry")

combined <- bind_rows(
  sector_index,
  movie_index2 |> mutate(rebased = Movie_Index)
)

```

```{r}
combined2 <- combined |>
  mutate(
    is_movie = (Sector == "Movie Industry")
  )

ggplot(combined2, aes(Date, rebased, color = Sector, group = Sector)) +
  geom_line(aes(linewidth = is_movie, alpha = is_movie)) +
  scale_y_log10() +
  scale_linewidth_manual(values = c("FALSE" = 0.6, "TRUE" = 2.0)) +
  scale_alpha_manual(values = c("FALSE" = 0.8, "TRUE" = 1.0)) +
  labs(
    title = "Movie Industry vs Major Sector ETFs (Rebased to 1)",
    subtitle = "Movie Industry highlighted for comparison",
    y = "Rebased Price (log scale)",
    x = "Date",
    color = "Sector / Industry",
    linewidth = NULL,
    alpha = NULL
  ) +
  theme_minimal() +
  theme(legend.position = "right")
```
Used average stock price of companies in the movie industry 


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)

# combined: Date, Sector, rebased (you already built this)
# Make sure it's sorted
combined2 <- combined |>
  arrange(Sector, Date)

# 1) Get final rebased value per Sector
final_perf <- combined2 |>
  group_by(Sector) |>
  filter(!is.na(rebased)) |>
  slice_tail(n = 1) |>
  ungroup() |>
  transmute(
    Sector,
    start = 1,               # everyone starts at 1
    end   = rebased          # ending multiple
  )

# Long format for slopegraph
perf_long <- final_perf |>
  pivot_longer(
    cols = c(start, end),
    names_to  = "Point",
    values_to = "value"
  ) |>
  mutate(
    Point = factor(Point, levels = c("start", "end")),
    is_movie = (Sector == "Movie Industry")
  )

ggplot(perf_long, aes(x = Point, y = value, group = Sector, color = Sector)) +
  # connecting line
  geom_line(aes(linewidth = is_movie, alpha = is_movie)) +
  # start/end points
  geom_point(size = 3, aes(size = is_movie)) +
  # label end with “×1.2”, “×2.3”, etc.
  geom_text(
    data = perf_long |> filter(Point == "end"),
    aes(
      label = paste0("×", round(value, 1)),
      hjust = -0.1
    ),
    size = 3
  ) +
  scale_x_discrete(
    labels = c(
      start = "2019 (rebased = 1)",
      end   = "2025 (final level)"
    )
  ) +
  scale_y_log10(labels = number_format(accuracy = 0.1)) +
  scale_linewidth_manual(values = c("FALSE" = 0.5, "TRUE" = 1.8)) +
  scale_alpha_manual(values = c("FALSE" = 0.5, "TRUE" = 1.0)) +
  scale_size_manual(values = c("FALSE" = 1.5, "TRUE" = 3.5), guide = "none") +
  coord_cartesian(clip = "off") +
  labs(
    title = "Growth Since 2019: Movie Industry vs Major Sectors",
    subtitle = "All sectors start at 1; end point shows final multiple (log scale)",
    x = "",
    y = "Rebased level (× starting value)",
    color = "Sector / Industry",
    linewidth = NULL,
    alpha = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "right",
    plot.margin = margin(5.5, 40, 5.5, 5.5) # space for labels on right
  )
```
```{r}

library(dplyr)
library(ggplot2)
library(forcats)
library(scales)

# 1) Get final rebased level for each Sector
final_perf <- combined |>
  arrange(Sector, Date) |>
  group_by(Sector) |>
  filter(!is.na(rebased)) |>
  slice_tail(n = 1) |>
  ungroup() |>
  transmute(
    Sector,
    multiple = rebased   # how many times the 2019 starting value
  ) |>
  mutate(
    Sector = fct_reorder(Sector, multiple),
    is_movie = (Sector == "Movie Industry"),
    label = paste0("×", round(multiple, 1))
  )

ggplot(final_perf, aes(x = Sector, y = multiple, fill = is_movie)) +
  geom_col() +
  geom_text(
    aes(label = label),
    hjust = -0.1,
    size = 3
  ) +
  coord_flip(clip = "off") +
  scale_fill_manual(
    values = c("FALSE" = "grey70", "TRUE" = "#d73027"),
    guide = "none"
  ) +
  scale_y_continuous(
    expand = expansion(mult = c(0, 0.15))
  ) +
  labs(
    title = "Cumulative Performance Since 2019",
    subtitle = "Each bar shows final rebased level (2019 = 1)",
    x = "",
    y = "Rebased level (× starting value)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.margin = margin(5.5, 40, 5.5, 5.5)
  )
```
This bar chart compares how much each major U.S. sector and the movie industry have grown since 2019 (in “× starting value” terms), highlighting that the movie industry’s cumulative performance has lagged behind most other sectors.


### 3. Performance of sub-industries within the movie industry differ
```{r}
media_rebased <- media |>
  group_by(Ticker) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    base = first(Adj_Close),
    rebased = Adj_Close / base
  ) |>
  ungroup()

ggplot(media_rebased, aes(Date, rebased, color = Ticker)) +
  geom_line(alpha = 0.85) +
  scale_y_log10() +
  labs(
    title = "Long-Term Performance of Movie/Media Companies vs S&P 500",
    subtitle = "Rebased to 1 at first date; log scale highlights relative divergence",
    y = "Rebased Price (log scale)",
    x = "Date"
  ) +
  theme_minimal()
```
```{r}
media_rebased <- media |>
  filter(!is.na(Adj_Close)) |>
  group_by(Ticker, SubIndustry) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    base    = first(Adj_Close),
    rebased = Adj_Close / base
  ) |>
  ungroup()

p2 <- ggplot(media_rebased, aes(Date, rebased, color = Ticker)) +
  geom_line(alpha = 0.85) +
  facet_wrap(~ SubIndustry, scales = "free_y") +
  labs(
    title = "Sub-Industry Performance: Theaters vs Studios vs Streamers vs Diversified vs Benchmark",
    subtitle = "Each ticker rebased to 1 at its first non-missing price (log scale)",
    y = "Rebased Price",
    x = "Date"
  ) +
  theme_minimal()

print(p2)
```


```{r}
# -------------------------
# Rebase individual stocks
# -------------------------
media_rebased <- media |>
  filter(!is.na(Adj_Close)) |>
  group_by(Ticker) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    base = first(Adj_Close),
    rebased = Adj_Close / base
  ) |> 
  ungroup()

# -------------------------
# Equal-weight subindustry index
# -------------------------
subindustry_index <- media_rebased |>
  group_by(Date, SubIndustry) |>
  summarize(SubIndex = mean(rebased, na.rm = TRUE), .groups = "drop")

# -------------------------
# Wide matrix
# -------------------------
sub_wide <- subindustry_index |>
  pivot_wider(names_from = SubIndustry, values_from = SubIndex)

# -------------------------
# Desired order
# -------------------------
order_vec <- c("Benchmark", "Streamer", "Diversified", "Studio", "Theater")

# -------------------------
# Correlation matrix with reordered columns
# -------------------------
cor_mat <- sub_wide |>
  select(all_of(order_vec)) |>
  cor(use = "pairwise.complete.obs")

# -------------------------
# Tidy long format
# -------------------------
cor_long <- cor_mat |>
  as.data.frame() |>
  rownames_to_column("Var1") |>
  pivot_longer(cols = order_vec, names_to = "Var2", values_to = "Correlation")

# -------------------------
# Plot heatmap
# -------------------------
ggplot(cor_long, aes(Var1, Var2, fill = Correlation)) +
  geom_tile(color = "white", linewidth = 0.3) +
  scale_fill_viridis_c(option = "C", limits = c(-1, 1)) +
  labs(
    title = "Correlation Between Movie/Media Sub-Industries (2015–2025)",
    subtitle = "Clearer block structure after business-driven ordering",
    x = "", y = "", fill = "Correlation"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )

```

```{r}
# 1) Equal-weight subindustry index (rebased)
sub_index <- media |>
  filter(!is.na(Adj_Close)) |>
  group_by(Ticker) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    base = first(Adj_Close),
    rebased = Adj_Close / base
  ) |>
  ungroup() |>
  group_by(Date, SubIndustry) |>
  summarize(
    SubIndex = mean(rebased, na.rm = TRUE),
    .groups = "drop"
  )

# 2) Drawdown per subindustry
sub_dd <- sub_index |>
  group_by(SubIndustry) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    roll_max = cummax(SubIndex),
    drawdown = SubIndex / roll_max - 1
  ) |>
  ungroup()

ggplot(sub_dd, aes(Date, drawdown, color = SubIndustry)) +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 0.4) +
  geom_line(linewidth = 1) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Sub-Industry Drawdowns from Prior Peak",
    subtitle = "Theaters suffer persistent deep drawdowns; streamers and diversified recover more",
    x = "Date",
    y = "Drawdown from previous peak"
  ) +
  theme_minimal()

```
```{r}
time_underwater <- sub_dd |>
  mutate(underwater = drawdown < 0) |>
  group_by(SubIndustry) |>
  summarize(
    pct_underwater = mean(underwater, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(time_underwater, aes(fct_reorder(SubIndustry, pct_underwater), pct_underwater)) +
  geom_col() +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Percent of Days Spent Below Previous Peak",
    subtitle = "Structural damage is largest in Theaters; Benchmark and Diversified fare best",
    x = "",
    y = "% of trading days underwater"
  ) +
  theme_minimal()

```




```{r}
# --- Equal-weight subindustry returns (wide) ---
sub_returns <- media |>
  filter(!is.na(return)) |>
  group_by(Date, SubIndustry) |>
  summarize(sub_ret = mean(return, na.rm = TRUE), .groups = "drop") |>
  pivot_wider(names_from = SubIndustry, values_from = sub_ret) |>
  drop_na()

# --- PCA matrix ---
X <- sub_returns |>
  select(-Date) |>
  as.matrix()

# --- PCA ---
pca <- prcomp(X, scale. = TRUE)

# --- Loadings (important part) ---
loadings <- as.data.frame(pca$rotation) |>
  rownames_to_column("SubIndustry")

# scale arrows for plotting aesthetics
arrow_scale <- 6
loadings_plot <- loadings |>
  mutate(
    PC1 = PC1 * arrow_scale,
    PC2 = PC2 * arrow_scale
  )

# --- Clean biplot with arrows only ---
ggplot(loadings_plot, aes(PC1, PC2)) +
  geom_segment(aes(x = 0, y = 0, xend = PC1, yend = PC2),
               arrow = arrow(length = unit(0.25, "cm")),
               linewidth = 1,
               color = "red") +
  geom_text(aes(label = SubIndustry),
            vjust = -0.5, hjust = 0.5, size = 5, fontface = "bold") +
  labs(
    title = "PCA of Movie/Media Subindustry Daily Returns",
    subtitle = "Arrows show subindustry loadings on PC1 and PC2",
    x = paste0("PC1 (", round(summary(pca)$importance[2,1]*100,1), "% variance)"),
    y = paste0("PC2 (", round(summary(pca)$importance[2,2]*100,1), "% variance)")
  ) +
  theme_minimal(base_size = 14)
```
```{r}
library(ggridges)

# 2. Build subindustry daily returns (this defines sub_daily) ----
sub_daily <- media |>
  filter(!is.na(return)) |>
  group_by(Date, SubIndustry) |>
  summarize(
    sub_ret = mean(return, na.rm = TRUE),
    .groups = "drop"
  )

# 3. Ridgeline plot, zoomed around typical moves (e.g. -8% to +8%) ----
ggplot(sub_daily, aes(x = sub_ret, y = SubIndustry, fill = SubIndustry)) +
  geom_density_ridges(alpha = 0.8, scale = 4) +
  scale_x_continuous(
    labels = scales::percent,
    limits = c(-0.08, 0.08)   # zoom in: change to c(-0.05, 0.05) if you want tighter
  ) +
  labs(
    title = "Density of Daily Returns by Subindustry",
    subtitle = "Zoomed in on typical daily moves (±8%)",
    x = "Daily Return",
    y = ""
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```




```{r}
library(dplyr)
library(ggplot2)
library(ggalluvial)
library(lubridate)
library(forcats)

# 1) Add regime labels -----------------------------------------------------
media_regime <- media |>
  mutate(
    Date = as.Date(Date),
    regime = case_when(
      Date < as.Date("2020-02-20") ~ "Pre-COVID",
      Date >= as.Date("2020-02-20") & Date <= as.Date("2021-12-31") ~ "COVID & stimulus",
      Date >= as.Date("2022-01-01") & Date <= as.Date("2022-12-31") ~ "Rate hikes / inflation",
      Date >= as.Date("2023-01-01") ~ "AI / recent",
      TRUE ~ NA_character_
    )
  ) |>
  filter(!is.na(regime), !is.na(return))

# 2) CUMULATIVE return per Ticker × SubIndustry × Regime ------------------
regime_perf <- media_regime |>
  group_by(Ticker, SubIndustry, regime) |>
  summarize(
    cum_ret = prod(1 + return, na.rm = TRUE) - 1,  # (1+r1)*(1+r2)*... - 1
    .groups = "drop"
  )

# 3) Bucket each ticker into Low / Mid / High within each regime ----------
regime_buckets <- regime_perf |>
  group_by(regime) |>
  mutate(
    q1 = quantile(cum_ret, 0.33, na.rm = TRUE),
    q2 = quantile(cum_ret, 0.67, na.rm = TRUE),
    bucket = case_when(
      cum_ret <= q1 ~ "Low",
      cum_ret >= q2 ~ "High",
      TRUE          ~ "Mid"
    )
  ) |>
  ungroup() |>
  mutate(
    bucket = factor(bucket, levels = c("Low", "Mid", "High"))
  )

# 4) Order regimes for the x-axis -----------------------------------------
regime_levels <- c("Pre-COVID", "COVID & stimulus",
                   "Rate hikes / inflation", "AI / recent")

flow_df <- regime_buckets |>
  filter(regime %in% regime_levels) |>
  mutate(
    regime = factor(regime, levels = regime_levels)
  )

# 5) Alluvial plot: ticker flows between buckets across regimes -----------
ggplot(
  flow_df,
  aes(x = regime,
      stratum = bucket,
      alluvium = Ticker,
      y = 1,
      fill = SubIndustry)
) +
  geom_flow(stat = "alluvium",
            lode.guidance = "forward",
            color = "grey50",
            alpha = 0.6) +
  geom_stratum(color = "black") +
  geom_text(
    stat = "stratum",
    aes(label = after_stat(stratum)),
    size = 3,
    color = "black"
  ) +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    title = "How Movie/Media Stocks Move Across Performance Buckets Over Time",
    subtitle = "Each stream is a ticker; Low / Mid / High based on cumulative return in each regime",
    x = "Regime",
    y = "Number of tickers",
    fill = "Subindustry"
  ) +
  theme_minimal()
```




```{r}
library(dplyr)
library(ggplot2)
library(scales)

regime_order <- c("Pre-COVID",
                  "COVID & stimulus",
                  "Rate hikes / inflation",
                  "AI / recent")

regime_sub <- media_regime |>
  group_by(SubIndustry, regime) |>
  summarize(
    cum_ret = prod(1 + return, na.rm = TRUE) - 1,
    .groups = "drop"
  ) |>
  mutate(
    regime = factor(regime, levels = regime_order)
  )

ggplot(regime_sub,
       aes(x = regime, y = cum_ret, fill = SubIndustry)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Cumulative Return by Subindustry and Regime",
    subtitle = "COVID & stimulus period supercharged streamers; theaters lag in later regimes",
    x = "",
    y = "Cumulative return in regime"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```




### 4. Netflix behaves like a tech company & AMC theatre underperforms 
```{r}
# 1) Movie industry equal-weight price index (use Adj_Close, not rebased)
movie_index <- media |>
  filter(SubIndustry != "Benchmark") |>
  group_by(Date) |>
  summarize(
    movie_price = mean(Adj_Close, na.rm = TRUE),
    .groups = "drop"
  )

# 2) S&P 500
sp <- media |>
  filter(Ticker == "^GSPC") |>
  select(Date, sp_price = Adj_Close)

# 3) Netflix
nflx <- media |>
  filter(Ticker == "NFLX") |>
  select(Date, nflx_price = Adj_Close)

# 4) Align dates and rebase all three to 1 at common start
all3 <- movie_index |>
  inner_join(sp, by = "Date") |>
  inner_join(nflx, by = "Date") |>
  arrange(Date)

base_movie <- first(all3$movie_price)
base_sp    <- first(all3$sp_price)
base_nflx  <- first(all3$nflx_price)

relative_long <- all3 |>
  mutate(
    `Movie Industry` = movie_price / base_movie,
    `S&P 500`        = sp_price    / base_sp,
    `Netflix`        = nflx_price  / base_nflx
  ) |>
  select(Date, `Movie Industry`, `S&P 500`, `Netflix`) |>
  pivot_longer(
    cols = -Date,
    names_to = "Series",
    values_to = "rebased"
  )

ggplot(relative_long, aes(Date, rebased, color = Series)) +
  geom_line(linewidth = 1) +
  scale_y_log10() +
  labs(
    title = "Netflix vs Movie Industry vs S&P 500 (Rebased)",
    subtitle = "Netflix tracks the broad market more closely than the struggling movie industry",
    x = "Date",
    y = "Rebased Price (log scale)",
    color = ""
  ) +
  theme_minimal()
```

```{r}
library(dplyr)
library(ggplot2)

focus_tickers <- c("NFLX", "ROKU", "AMC", "DIS", "^GSPC")

media_focus <- media |>
  filter(Ticker %in% focus_tickers, !is.na(Adj_Close)) |>
  group_by(Ticker) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    base    = first(Adj_Close),
    rebased = Adj_Close / base
  ) |>
  ungroup()

ggplot(media_focus,
       aes(x = Date, y = rebased, color = Ticker)) +
  geom_line(linewidth = 0.9, alpha = 0.9) +
  scale_y_log10() +
  labs(
    title = "Rebased Performance of Selected Movie/Media Stocks",
    subtitle = "Each ticker rebased to 1 at its first non-missing price (log scale)",
    x = "Date",
    y = "Rebased price (log scale)"
  ) +
  theme_minimal()
```

```{r}
library(dplyr)
library(ggplot2)
library(scales)

# compute metrics for the same tickers
stock_stats <- media |>
  filter(Ticker %in% focus_tickers, !is.na(return)) |>
  group_by(Ticker) |>
  arrange(Date, .by_group = TRUE) |>
  mutate(
    rebased   = cumprod(1 + return),          # grows from 1
    roll_max  = cummax(rebased),
    drawdown  = rebased / roll_max - 1
  ) |>
  summarize(
    cum_ret      = last(rebased) - 1,
    ann_ret      = (1 + mean(return, na.rm = TRUE))^252 - 1,
    ann_vol      = sd(return, na.rm = TRUE) * sqrt(252),
    max_drawdown = min(drawdown, na.rm = TRUE),
    .groups = "drop"
  )

stock_stats
```

```{r}
ggplot(stock_stats,
       aes(x = Ticker, y = max_drawdown)) +
  geom_col() +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Max Drawdown for Selected Stocks",
    x = "",
    y = "Max drawdown from prior peak"
  ) +
  theme_minimal()
```


```{r}
library(dplyr)
library(ggplot2)
library(scales)

regime_order <- c("Pre-COVID",
                  "COVID & stimulus",
                  "Rate hikes / inflation",
                  "AI / recent")

regime_focus <- media_regime |>
  filter(Ticker %in% focus_tickers,
         !is.na(return),
         !is.na(regime)) |>
  group_by(Ticker, regime) |>
  summarize(
    cum_ret = prod(1 + return, na.rm = TRUE) - 1,
    .groups = "drop"
  ) |>
  mutate(
    regime = factor(regime, levels = regime_order)
  )

ggplot(regime_focus,
       aes(x = regime, y = cum_ret, fill = Ticker)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Cumulative Return by Regime for Selected Stocks",
    x = "",
    y = "Cumulative return in regime"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```



alluvial diagram of movies distributed in theaters vs. streaming? 
compare netflix revenue to other companies? 
